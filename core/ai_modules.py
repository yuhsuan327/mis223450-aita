import os
import re
import whisper
import json
import warnings
from dotenv import load_dotenv
from openai import OpenAI
from .models import Lecture, Question
load_dotenv()


def transcribe_with_whisper(audio_path, model_size="small"):
    try:
        warnings.filterwarnings("ignore", message=".*FP16 is not supported on CPU.*")
        if not os.path.exists(audio_path):
            print(f"âŒ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆï¼š{audio_path}")
            return None
        model = whisper.load_model(model_size)
        print("âœ… Whisper è½‰éŒ„é–‹å§‹")
        result = model.transcribe(audio_path, fp16=False)
        return result["text"]
    except Exception as e:
        print(f"âŒ Whisper è½‰éŒ„éŒ¯èª¤: {e}")
        return None


def dynamic_split(text, min_length=300, max_length=1000):
    text = text.strip()
    if len(text) <= max_length:
        return [text]
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', text)
    chunks, temp = [], ""
    for para in paragraphs:
        if len(temp) + len(para) <= max_length:
            temp += para
        else:
            if len(temp) >= min_length:
                chunks.append(temp.strip())
                temp = para
            else:
                temp += para
    if temp:
        chunks.append(temp.strip())
    return chunks


def create_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    if not api_key or api_key.strip().upper() == "EMPTY":
        raise ValueError("âŒ è«‹è¨­å®š OPENAI_API_KEY")
    return OpenAI(api_key=api_key, base_url=api_base)


def generate_summary_for_chunk(client, chunk, chunk_index, total_chunks):
    prompt = [
        {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡èª²ç¨‹æ‘˜è¦è¨­è¨ˆå¸«ã€‚
è«‹é‡å°ç¬¬ {chunk_index + 1} æ®µèª²ç¨‹å…§å®¹é€²è¡Œé‡é»æ‘˜è¦ï¼ŒåŒ…å«ï¼š
- ç°¡æ½”å…§å®¹æ¦‚è¿°ï¼ˆ50â€“80å­—ï¼‰
- 2â€“4 å€‹å­¸ç¿’è¦é»ï¼Œä½¿ç”¨æ¢åˆ—å¼
ç¸½å­—æ•¸æ§åˆ¶åœ¨ 150 å­—å…§ã€‚"""},
        {"role": "user", "content": chunk}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=prompt,
            temperature=0.3,
            max_tokens=400
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ®µè½æ‘˜è¦éŒ¯èª¤: {e}")
        return f"ç¬¬ {chunk_index + 1} æ®µæ‘˜è¦å¤±æ•—"


def combine_summaries(client, summaries):
    combined = "\n\n".join([f"æ®µè½ {i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    prompt = [
        {"role": "system", "content": """ä½ æ˜¯æ•™è‚²è¨­è¨ˆå°ˆå®¶ï¼Œè«‹å°‡ä¸‹åˆ—åˆ†æ®µæ‘˜è¦çµ±æ•´ç‚ºå®Œæ•´èª²ç¨‹æ‘˜è¦ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

ã€èª²ç¨‹æ¦‚è¿°ã€‘ï¼šèªªæ˜æ•´é«”èª²ç¨‹å…§å®¹èˆ‡é‡è¦æ€§ï¼ˆ150â€“200å­—ï¼‰
ã€å­¸ç¿’é‡é»ã€‘ï¼šåˆ—å‡ºæœ¬èª²ç¨‹çš„ 4â€“5 å€‹å­¸ç¿’ç›®æ¨™ï¼ˆæ¢åˆ—ï¼‰
ã€å®Œæˆå¾Œæ”¶ç©«ã€‘ï¼šç°¡è¿°å­¸ç”Ÿå®Œæˆèª²ç¨‹å¾Œèƒ½å…·å‚™çš„èƒ½åŠ›ï¼ˆ60å­—å…§ï¼‰

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…é‡è¤‡æ•˜è¿°ï¼Œæ§åˆ¶ç¸½å­—æ•¸åœ¨ 400 å­—å…§ã€‚"""},
        {"role": "user", "content": combined}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=prompt,
            temperature=0.3,
            max_tokens=512
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¸½çµéŒ¯èª¤: {e}")
        return "æ•´åˆæ‘˜è¦å¤±æ•—"


def generate_quiz(client, summary, count=3):
    prompt = [
        {
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€ä½èª²ç¨‹å‡ºé¡Œ AIï¼Œè«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ç”¢ç”Ÿ {count} é¡Œé¸æ“‡é¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{
    "concept": "å­¸ç¿’æ¦‚å¿µ",
    "question": "å•é¡Œå…§å®¹",
    "options": {{
      "A": "é¸é … A",
      "B": "é¸é … B",
      "C": "é¸é … C",
      "D": "é¸é … D"
    }},
    "answer": "B",
    "explanation": "æ­£ç¢ºç­”æ¡ˆè§£æ"
  }},
  ...
]
è«‹ç”¨ JSON æ ¼å¼å›è¦†ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—èªªæ˜ã€‚"""
        },
        {"role": "user", "content": summary}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=prompt,
            temperature=0.5,
            max_tokens=1500
        )
        return json.loads(response.choices[0].message.content.strip())

    except Exception as e:
        print(f"âŒ é¸æ“‡é¡Œç”¢ç”Ÿå¤±æ•—ï¼š{e}")
        return []


def generate_tf_questions(client, summary, count):
    prompt = [
        {
            "role": "system",
            "content": f"""è«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ï¼Œè¨­è¨ˆ {count} é¡Œæ˜¯éé¡Œï¼ˆTrue/Falseï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{
    "concept": "å­¸ç¿’æ¦‚å¿µ",
    "question": "å•é¡Œå…§å®¹",
    "answer": "True",
    "explanation": "æ­£ç¢ºç­”æ¡ˆè§£æ"
  }},
  ...
]
è«‹å›å‚³ JSON æ ¼å¼è³‡æ–™ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—èªªæ˜ã€‚"""
        },
        {"role": "user", "content": summary}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=prompt,
            temperature=0.5
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âŒ æ˜¯éé¡Œç”¢ç”Ÿå¤±æ•—ï¼š{e}")
        return []


def parse_and_store_questions(summary, quiz_data, lecture, question_type):
    for item in quiz_data:
        question_text = item.get('question', '').strip()
        explanation = item.get('explanation', '').strip()
        answer = item.get('answer', '').strip()

        if question_type == 'mcq':
            options = item.get('options', {})
            Question.objects.create(
                lecture=lecture,
                question_text=question_text,
                option_a=options.get('A'),
                option_b=options.get('B'),
                option_c=options.get('C'),
                option_d=options.get('D'),
                correct_answer=answer,
                explanation=explanation,
                question_type='mcq'
            )
        elif question_type == 'tf':
                Question.objects.create(
                    lecture=lecture,
                    question_text=item.get('question', '').strip(),
                    correct_answer=item.get('answer'),
                    explanation=item.get('explanation', '').strip(),
                    question_type='tf'
            )
        else:
            print(f"âš ï¸ æœªçŸ¥é¡Œå‹ï¼š{question_type}")


def process_audio_and_generate_quiz(lecture_id, num_mcq=3, num_tf=0):
    lecture = Lecture.objects.get(id=lecture_id)
    client = create_openai_client()

    print("ğŸ§ é–‹å§‹èªéŸ³è½‰éŒ„")
    transcript = transcribe_with_whisper(lecture.audio_file.path)
    if not transcript:
        return
    lecture.transcript = transcript
    lecture.save()

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]

    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")

    if num_mcq > 0:
        mcq_data = generate_quiz(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")
